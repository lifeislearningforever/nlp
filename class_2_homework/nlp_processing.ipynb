{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuitive study about Processing of text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Tokenization is a particular kind of document segmentation. Segmentation breaks up text into smaller chunks or segments. \n",
    "Segmentation can can include breaking a document into paragraphs, paragraphs into sentences, sentences into phrases or tokens\n",
    "(usually words)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What is unstructured data in data science?\n",
    "From Wikipedia, the free encyclopedia. Unstructured data (or unstructured information) is information that either does not have a pre-defined data model or is not organized in a pre-defined manner. Unstructured information is typically text-heavy, but may contain data such as dates, numbers, and facts as wel"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What is tokenizer?\n",
    "A tokenizer breaks unstructured data, natural language text, into chunks of information that can be counted as discrete elemnents. These counts of token occurrences in a document can be used directly as vector representating that document. This immediately turns an unstructured string into numerical data structure suitable for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 'is',\n",
       " 'such',\n",
       " 'a',\n",
       " 'wonderful',\n",
       " 'day.',\n",
       " 'I',\n",
       " 'love',\n",
       " 'everything',\n",
       " 'about',\n",
       " 'learning!.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the easiest way to find boundary of a word ? or tokenize it? \n",
    "# using whitespace\n",
    "a_sentence = \"It is such a wonderful day. I love everything about learning!.\"\n",
    "a_sentence.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created our basic tokenizer. Can you tell me is this fine ? \n",
    "1, 1?, 1!,1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = a_sentence.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I, It, a, about, day., everything, is, learning!., love, such, wonderful'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \"\"\"Thanos is a fictional supervillain appearing in American comic books published by Marvel Comics.\"\"\"\n",
    "sentences += \"\"\"The character was created by writer-artist Jim Starlin, and made his first appearance in The Invincible Iron Man\"\"\"\n",
    "sentences += \"\"\"Infinity War, Thanos begins his crusade to collect all six Infinity Stones and bring balance to the universe.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thanos is a fictional supervillain appearing in American comic books published by Marvel Comics.The character was created by writer-artist Jim Starlin, and made his first appearance in The Invincible Iron ManInfinity War, Thanos begins his crusade to collect all six Infinity Stones and bring balance to the universe.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = sentences.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thanos is a fictional supervillain appearing in American comic books published by Marvel Comics',\n",
       " 'The character was created by writer-artist Jim Starlin, and made his first appearance in The Invincible Iron ManInfinity War, Thanos begins his crusade to collect all six Infinity Stones and bring balance to the universe',\n",
       " '']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = {}\n",
    "for sentence in sent:\n",
    "    tokens = sentence.split(\" \")\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Thanos': 2,\n",
       " 'is': 1,\n",
       " 'a': 1,\n",
       " 'fictional': 1,\n",
       " 'supervillain': 1,\n",
       " 'appearing': 1,\n",
       " 'in': 2,\n",
       " 'American': 1,\n",
       " 'comic': 1,\n",
       " 'books': 1,\n",
       " 'published': 1,\n",
       " 'by': 2,\n",
       " 'Marvel': 1,\n",
       " 'Comics': 1,\n",
       " 'The': 2,\n",
       " 'character': 1,\n",
       " 'was': 1,\n",
       " 'created': 1,\n",
       " 'writer-artist': 1,\n",
       " 'Jim': 1,\n",
       " 'Starlin,': 1,\n",
       " 'and': 2,\n",
       " 'made': 1,\n",
       " 'his': 2,\n",
       " 'first': 1,\n",
       " 'appearance': 1,\n",
       " 'Invincible': 1,\n",
       " 'Iron': 1,\n",
       " 'ManInfinity': 1,\n",
       " 'War,': 1,\n",
       " 'begins': 1,\n",
       " 'crusade': 1,\n",
       " 'to': 2,\n",
       " 'collect': 1,\n",
       " 'all': 1,\n",
       " 'six': 1,\n",
       " 'Infinity': 1,\n",
       " 'Stones': 1,\n",
       " 'bring': 1,\n",
       " 'balance': 1,\n",
       " 'the': 1,\n",
       " 'universe': 1,\n",
       " '': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] \n",
    "for sentence in sent: \n",
    "    vector = [] \n",
    "    for word in wordfreq: \n",
    "        if word in sentence.split(' '): \n",
    "            vector.append(1) \n",
    "        else: \n",
    "            vector.append(0) \n",
    "    X.append(vector) \n",
    "X = np.asarray(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.8/site-packages (1.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/anaconda3/lib/python3.8/site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/anaconda3/lib/python3.8/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X, columns = wordfreq.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Thanos</th>\n",
       "      <th>is</th>\n",
       "      <th>a</th>\n",
       "      <th>fictional</th>\n",
       "      <th>supervillain</th>\n",
       "      <th>appearing</th>\n",
       "      <th>in</th>\n",
       "      <th>American</th>\n",
       "      <th>comic</th>\n",
       "      <th>books</th>\n",
       "      <th>...</th>\n",
       "      <th>collect</th>\n",
       "      <th>all</th>\n",
       "      <th>six</th>\n",
       "      <th>Infinity</th>\n",
       "      <th>Stones</th>\n",
       "      <th>bring</th>\n",
       "      <th>balance</th>\n",
       "      <th>the</th>\n",
       "      <th>universe</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Thanos  is  a  fictional  supervillain  appearing  in  American  comic  \\\n",
       "0       1   1  1          1             1          1   1         1      1   \n",
       "1       1   0  0          0             0          0   1         0      0   \n",
       "2       0   0  0          0             0          0   0         0      0   \n",
       "\n",
       "   books  ...  collect  all  six  Infinity  Stones  bring  balance  the  \\\n",
       "0      1  ...        0    0    0         0       0      0        0    0   \n",
       "1      0  ...        1    1    1         1       1      1        1    1   \n",
       "2      0  ...        0    0    0         0       0      0        0    0   \n",
       "\n",
       "   universe     \n",
       "0         0  0  \n",
       "1         1  0  \n",
       "2         0  1  \n",
       "\n",
       "[3 rows x 43 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thanos is a fictional supervillain appearing in American comic books published by Marvel Comics',\n",
       " 'The character was created by writer-artist Jim Starlin, and made his first appearance in The Invincible Iron ManInfinity War, Thanos begins his crusade to collect all six Infinity Stones and bring balance to the universe',\n",
       " '']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We will study about various tokenizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (4.47.0)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (2020.6.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.text import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thanos',\n",
       " 'is',\n",
       " 'a',\n",
       " 'fictional',\n",
       " 'supervillain',\n",
       " 'appearing',\n",
       " 'in',\n",
       " 'american',\n",
       " 'comic',\n",
       " 'books',\n",
       " 'published',\n",
       " 'by',\n",
       " 'marvel',\n",
       " 'comics.the',\n",
       " 'character',\n",
       " 'was',\n",
       " 'created',\n",
       " 'by',\n",
       " 'writer-artist',\n",
       " 'jim',\n",
       " 'starlin',\n",
       " ',',\n",
       " 'and',\n",
       " 'made',\n",
       " 'his',\n",
       " 'first',\n",
       " 'appearance',\n",
       " 'in',\n",
       " 'the',\n",
       " 'invincible',\n",
       " 'iron',\n",
       " 'maninfinity',\n",
       " 'war',\n",
       " ',',\n",
       " 'thanos',\n",
       " 'begins',\n",
       " 'his',\n",
       " 'crusade',\n",
       " 'to',\n",
       " 'collect',\n",
       " 'all',\n",
       " 'six',\n",
       " 'infinity',\n",
       " 'stones',\n",
       " 'and',\n",
       " 'bring',\n",
       " 'balance',\n",
       " 'to',\n",
       " 'the',\n",
       " 'universe',\n",
       " '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(sentences)\n",
    "tokens = [word.lower() for word in tokens]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check out various tokenizers on https://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: thanos is a fictional supervillain appearing in american...>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Text(tokens)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 2 of 2 matches:\n",
      " thanos is a fictional supervillain appeari\n",
      "e invincible iron maninfinity war , thanos begins his crusade to collect all s\n"
     ]
    }
   ],
   "source": [
    "t.concordance('thanos')\n",
    "# def collocations(self, num=20, window_size=2). num is the max no. of collocations to print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.count('thanos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.index('fictional')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"A bank is a financial institution that accepts deposits from the public and creates a demand deposit while simultaneously making loans.[1] Lending activities can be performed either directly or indirectly through capital markets.\n",
    "\n",
    "Due to the importance of banks in the financial stability of a country, most jurisdictions exercise a high degree of regulation over banks. Most countries have institutionalized a system known as fractional reserve banking, under which banks hold liquid assets equal to only a portion of their current liabilities. In addition to other regulations intended to ensure liquidity, banks are generally subject to minimum capital requirements based on an international set of capital standards, the Basel Accords.\n",
    "\n",
    "Banking in its modern sense evolved in the fourteenth century in the prosperous cities of Renaissance Italy but in many ways functioned as a continuation of ideas and concepts of credit and lending that had their roots in the ancient world. In the history of banking, a number of banking dynasties — notably, the Medicis, the Fuggers, the Welsers, the Berenbergs, and the Rothschilds — have played a central role over many centuries. The oldest existing retail bank is Banca Monte dei Paschi di Siena (founded in 1472), while the oldest existing merchant bank is Berenberg Bank (founded in 1590)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = word_tokenize(text)\n",
    "t = Text(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.8/site-packages (3.2.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (1.18.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'bank', 'is', 'a', 'financial', 'institution', 'that', 'accepts']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t[0:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t.dispersion_plot(['bank', 'many', 'financial']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t.plot(20) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 14, ',': 11, 'of': 10, 'a': 9, 'in': 8, '.': 7, 'to': 5, 'and': 4, 'banks': 4, 'bank': 3, ...})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stopwords Corpus  This corpus contains lists of stop words for several languages.  These are high-frequency grammatical words which are usually ignored in text retrieval applications.  They were obtained from: http://anoncvs.postgresql.org/cvsweb.cgi/pgsql/src/backend/snowball/stopwords/  The stop words for the Romanian language were obtained from: http://arlc.ro/resources/  The English list has been augmented https://github.com/nltk/nltk_data/issues/22  The German list has been corrected https://github.com/nltk/nltk_data/pull/49  A Kazakh list has been added https://github.com/nltk/nltk_data/pull/52  A Nepali list has been added https://github.com/nltk/nltk_data/pull/83  An Azerbaijani list has been added https://github.com/nltk/nltk_data/pull/100  A Greek list has been added https://github.com/nltk/nltk_data/pull/103  An Indonesian list has been added https://github.com/nltk/nltk_data/pull/112 '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.readme().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words(['english', 'greek']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thanos is a fictional supervillain appearing in American comic books published by Marvel Comics',\n",
       " 'The character was created by writer-artist Jim Starlin, and made his first appearance in The Invincible Iron ManInfinity War, Thanos begins his crusade to collect all six Infinity Stones and bring balance to the universe',\n",
       " '']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = {}\n",
    "for sentence in sent:\n",
    "    tokens = word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        if token not in stw:\n",
    "            if token not in wordfreq.keys():\n",
    "                wordfreq[token] = 1\n",
    "            else:\n",
    "                wordfreq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] \n",
    "for sentence in sent: \n",
    "    vector = [] \n",
    "    for word in wordfreq: \n",
    "        if word not in stw:\n",
    "            if word in sentence.split(' '): \n",
    "                vector.append(1) \n",
    "            else: \n",
    "                vector.append(0) \n",
    "    X.append(vector) \n",
    "X = np.asarray(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X, columns = wordfreq.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Thanos</th>\n",
       "      <th>fictional</th>\n",
       "      <th>supervillain</th>\n",
       "      <th>appearing</th>\n",
       "      <th>American</th>\n",
       "      <th>comic</th>\n",
       "      <th>books</th>\n",
       "      <th>published</th>\n",
       "      <th>Marvel</th>\n",
       "      <th>Comics</th>\n",
       "      <th>...</th>\n",
       "      <th>War</th>\n",
       "      <th>begins</th>\n",
       "      <th>crusade</th>\n",
       "      <th>collect</th>\n",
       "      <th>six</th>\n",
       "      <th>Infinity</th>\n",
       "      <th>Stones</th>\n",
       "      <th>bring</th>\n",
       "      <th>balance</th>\n",
       "      <th>universe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Thanos  fictional  supervillain  appearing  American  comic  books  \\\n",
       "0       1          1             1          1         1      1      1   \n",
       "1       1          0             0          0         0      0      0   \n",
       "2       0          0             0          0         0      0      0   \n",
       "\n",
       "   published  Marvel  Comics  ...  War  begins  crusade  collect  six  \\\n",
       "0          1       1       1  ...    0       0        0        0    0   \n",
       "1          0       0       0  ...    0       1        1        1    1   \n",
       "2          0       0       0  ...    0       0        0        0    0   \n",
       "\n",
       "   Infinity  Stones  bring  balance  universe  \n",
       "0         0       0      0        0         0  \n",
       "1         1       1      1        1         1  \n",
       "2         0       0      0        0         0  \n",
       "\n",
       "[3 rows x 33 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homework 1 - create your corpus \n",
    "# homework 2 - Extract the features\n",
    "# homework 3 - convert the text into bow model or X (numerical format) which can be later fed to model\n",
    "# homework 4 - create a greek corpus \n",
    "# homework 5 - write a program to find the language of the corpus \n",
    "# homework 6 - in previous homework 3, modify it and remove stopwords"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We will  study about various lemmatizers and stemmers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Languages we speak and write are made up of several words often derived from one another. When a language contains words that are derived from another word as their use in the speech changes is called Inflected Languag"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"In grammar, inflection is the modification of a word to express different grammatical categories such as tense, case, voice, aspect, person, number, gender, and mood. An inflection expresses one or more grammatical categories with a prefix, suffix or infix, or another internal modification such as a vowel change\" [Wikipedia]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The degree of inflection may be higher or lower in a language. As you have read the definition of inflection with respect to grammar, you can understand that an inflected word(s) will have a common root form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Python Stemming?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Python Stemming is the act of taking a word and reducing it into a stem. A stem is like a root for a word- that for writing is writing. But this doesn’t always have to be a word; words like study, studies, and studying all stem into the word studi, which isn’t actually a word.\n",
    "\"Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language.\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Applications of stemming : \n",
    " \n",
    "\n",
    "Stemming is used in information retrieval systems like search engines.\n",
    "It is used to determine domain vocabularies in domain analysis.\n",
    "Fun Fact: Google search adopted a word stemming in 2003. Previously a search for “fish” would not have returned “fishing” or “fishes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = test_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_stemmer(word):\n",
    "    if word[-2:] == 'es':\n",
    "        word = word[0:-2] \n",
    "    if word[-3:] == 'ing':\n",
    "        word = word[0:-3] \n",
    "    if word[-3:] == 'ner':\n",
    "        word = word[:-3] \n",
    "    return word\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial : buses ; stem :bus\n",
      "initial : houses ; stem :hous\n",
      "initial : runner ; stem :run\n",
      "initial : stunner ; stem :stun\n",
      "initial : running ; stem :runn\n",
      "initial : bringing ; stem :bring\n",
      "initial : stunning ; stem :stunn\n",
      "initial : fishes ; stem :fish\n",
      "initial : kitties ; stem :kitti\n"
     ]
    }
   ],
   "source": [
    "test_words = ['buses','houses','runner','stunner','running','bringing','stunning','fishes','kitties']\n",
    "for w in test_words:\n",
    "    print(\"initial : {} ; stem :{}\".format(w,my_stemmer(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Stemming algorithms are:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    "\n",
    "Porter’s Stemmer algorithm \n",
    "------------------------\n",
    "It is one of the most popular stemming methods proposed in 1980. It is based on the idea that the suffixes in the English language are made up of a combination of smaller and simpler suffixes. This stemmer is known for its speed and simplicity. The main applications of Porter Stemmer include data mining and Information retrieval. However, its applications are only limited to English words. Also, the group of stems is mapped on to the same stem and the output stem is not necessarily a meaningful word. The algorithms are fairly lengthy in nature and are known to be the oldest stemmer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Advantage: It produces the best output as compared to other stemmers and it has less error rate.\n",
    "Limitation:  Morphological variants produced are not always real words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lovins Stemmer \n",
    "-----------\n",
    "It is proposed by Lovins in 1968, that removes the longest suffix from a word then the word is recoded to convert this stem into valid words. \n",
    "Example: sitting -> sitt -> sit"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Advantage: It is fast and handles irregular plurals like 'teeth' and 'tooth' etc.\n",
    "Limitation: It is time consuming and frequently fails to form words from stem."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Dawson Stemmer \n",
    "---------------\n",
    "It is an extension of Lovins stemmer in which suffixes are stored in the reversed order indexed by their length and last letter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage: It is fast in execution and covers more suffices.\n",
    "Limitation: It is very complex to implement"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Krovetz Stemmer\n",
    "----------------\n",
    "It was proposed in 1993 by Robert Krovetz. Following are the steps: \n",
    "1) Convert the plural form of a word to its singular form. \n",
    "2) Convert the past tense of a word to its present tense and remove the suffix ‘ing’. \n",
    "Example: ‘children’ -> ‘child’"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Advantage: It is light in nature and can be used as pre-stemmer for other stemmers.\n",
    "Limitation: It is inefficient in case of large documents"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Snowball Stemmer:\n",
    "-------------------\n",
    "When compared to the Porter Stemmer, the Snowball Stemmer can map non-English words too. Since it supports other languages the Snowball Stemmers can be called a multi-lingual stemmer. The Snowball stemmers are also imported from the nltk package. This stemmer is based on a programming language called ‘Snowball’ that processes small strings and is the most widely used stemmer. The Snowball stemmer is way more aggressive than Porter Stemmer and is also referred to as Porter2 Stemmer. Because of the improvements added when compared to the Porter Stemmer, the Snowball stemmer is having greater computational speed. \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Lancaster Stemmer:\n",
    "-----------------\n",
    "The Lancaster stemmers are more aggressive and dynamic compared to the other two stemmers. The stemmer is really faster, but the algorithm is really confusing when dealing with small words. But they are not as efficient as Snowball Stemmers. The Lancaster stemmers save the rules externally and basically uses an iterative algorithm.\n",
    "LancasterStemmer is simple, but heavy stemming due to iterations and over-stemming may occur. Over-stemming causes the stems to be not linguistic, or they may have no meaning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Extra stemmer tests can be found in nltk.test.unit.test_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write: write\n",
      "writer: writer\n",
      "writing: write\n",
      "writers: writer\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "words=['write','writer','writing','writers']\n",
    "ps=PorterStemmer()\n",
    "for word in words:\n",
    "          print(f\"{word}: {ps.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\n",
    "            'plotted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "singles = [ps.stem(plural) for plural in plurals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caress',\n",
       " 'fli',\n",
       " 'die',\n",
       " 'mule',\n",
       " 'deni',\n",
       " 'die',\n",
       " 'agre',\n",
       " 'own',\n",
       " 'humbl',\n",
       " 'size',\n",
       " 'meet',\n",
       " 'state',\n",
       " 'siez',\n",
       " 'item',\n",
       " 'sensat',\n",
       " 'tradit',\n",
       " 'refer',\n",
       " 'colon',\n",
       " 'plot']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(SnowballStemmer.languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gener\n"
     ]
    }
   ],
   "source": [
    "print(SnowballStemmer(\"porter\").stem(\"generously\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous\n"
     ]
    }
   ],
   "source": [
    "print(SnowballStemmer(\"english\").stem(\"generously\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "troubl\n",
      "troubl\n",
      "troubl\n"
     ]
    }
   ],
   "source": [
    "print(lancaster.stem(\"cats\"))\n",
    "print(lancaster.stem(\"trouble\"))\n",
    "print(lancaster.stem(\"troubling\"))\n",
    "print(lancaster.stem(\"troubled\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is lemmatization?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Python Lemmatization lets us group together inflected forms of a word. It links words with similar meanings to one word and maps various words onto one root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'was',\n",
       " 'running',\n",
       " 'and',\n",
       " 'eating',\n",
       " 'at',\n",
       " 'same',\n",
       " 'time',\n",
       " 'He',\n",
       " 'has',\n",
       " 'bad',\n",
       " 'habit',\n",
       " 'of',\n",
       " 'swimming',\n",
       " 'after',\n",
       " 'playing',\n",
       " 'long',\n",
       " 'hours',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Sun']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 ha                  \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hours               hour                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the above output, you must be wondering that no actual root form has been given for any word, this is because they are given without context. You need to provide the context in which you want to lemmatize that is the parts-of-speech (POS). This is done by giving the value for pos parameter in wordnet_lemmatizer.lemmatize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\")))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization are itself form of NLP and widely used in Text mining. Text Mining is the process of analysis of texts written in natural language and extract high-quality information from text. It involves looking for interesting patterns in the text or to extract data from the text to be inserted into a database. Text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities). Developers have to prepare text using lexical analysis, POS (Parts-of-speech) tagging, stemming and other Natural Language Processing techniques to gain useful information from text."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Information Retrieval (IR) Environments:\n",
    "----------------------------------------------\n",
    "It is useful to use stemming and lemmatization to map documents to common topics and display search results by indexing when documents are increasing to mind-boggling numbers. Query Expansion is a term used in Search Environments which refers to that when a user inputs a query. It is used to expand or enhance the query to match additional documents.\n",
    "\n",
    "Stemming has been used in Query systems such as Web Search Engines, but due to problems of under-stemming and over-stemming it's effectiveness in returning correct results have been found limited. For example, a person searching for 'marketing' may not be pleased with results that will show 'markets' and not marketing. But Stemming may be found useful in other languages and using different algorithms for stemming may result in better outputs. Google search adopted stemming in 2003"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sentiment Analysis\n",
    "------------------\n",
    "Sentiment Analysis is the analysis of people's reviews and comments about something. It is widely used for analysis of product on online retail shops. Stemming and Lemmatization is used as part of the text-preparation process before it is analyzed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Document Clustering\n",
    "--------------------\n",
    "Document clustering (or text clustering) is the application of cluster analysis to textual documents. It has applications in an automatic document organization, topic extraction, and fast information retrieval or filtering. Examples of document clustering include web document clustering for search engines. Before Clustering methods are applied document is prepared through tokenization, removal of stop words and then Stemming and Lemmatization to reduce the number of tokens that carry out the same information and hence speed up the whole process. After this pre-processing, features are calculated by calculating the frequency of all tokens and then clustering methods are applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming or Lemmatizing?\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word.\n",
    "\n",
    "Stemming follows an algorithm with steps to perform on the words which makes it faster. Whereas, in lemmatization, you used WordNet corpus and a corpus for stop words as well to produce lemma which makes it slower than stemming. You also had to define a parts-of-speech to obtain the correct lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'indetifi'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('indetify')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'identify'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('identify')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homework 1 - create 10 files with 10 distinct subjects and name them\n",
    "# convert it into corpus\n",
    "# do pre processing on the text\n",
    "# take a word as input \n",
    "# and return the document that contains that word maximum times\n",
    "# take a query as input \n",
    "# and return the document that contains that query or have the best match comparitively"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So we also in first class learnt a lot of basic processing like split(), replace(),in, file   etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Regular Expressions (sometimes called regex for short) allow a user to search for strings using almost any sort of rule they can come up with. For example, finding all capital letters in a string, or finding a phone number in a document.\n",
    "\n",
    "Regular expressions are notorious for their seemingly strange syntax. This strange syntax is a byproduct of their flexibility. Regular expressions have to be able to filter out any string pattern you can imagine, which is why they have a complex string pattern format.\n",
    "\n",
    "Regular expressions are handled using Python's built-in re library. See the docs for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching for basic pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The agent's phone number is 408-555-1234. Call soon!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'phone' in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# this is pre built library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = 'phone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(12, 17), match='phone'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"NOT IN TEXT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(pattern,text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've seen that re.search() will take the pattern, scan the text, and then returns a Match object. If no pattern is found, a None is returned (in Jupyter Notebook this just means that nothing is output below the cell).\n",
    "\n",
    "Let's take a closer look at this Match object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = re.search('phone',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 17)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match.span()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"my phone is a new phone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = re.search(\"phone\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match.span()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you see here it is giving me the first match only and not all the matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = re.findall('phone',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(match)\n",
    "# tell you how many times a thing has occurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.count('phone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(3, 8), match='phone'>\n",
      "phone\n",
      "(3, 8)\n",
      "3\n",
      "8\n",
      "<re.Match object; span=(18, 23), match='phone'>\n",
      "phone\n",
      "(18, 23)\n",
      "18\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "for m in re.finditer('phone',text):\n",
    "    print(m)\n",
    "    print(m.group())\n",
    "    print(m.span())\n",
    "    print(m.start())\n",
    "    print(m.end())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table ><tr><th>Character</th><th>Description</th><th>Example Pattern Code</th><th >Exammple Match</th></tr>\n",
    "\n",
    "<tr ><td><span >\\d</span></td><td>A digit</td><td>file_\\d\\d</td><td>file_25</td></tr>\n",
    "\n",
    "<tr ><td><span >\\w</span></td><td>Alphanumeric</td><td>\\w-\\w\\w\\w</td><td>A-b_1</td></tr>\n",
    "\n",
    "\n",
    "\n",
    "<tr ><td><span >\\s</span></td><td>White space</td><td>a\\sb\\sc</td><td>a b c</td></tr>\n",
    "\n",
    "\n",
    "\n",
    "<tr ><td><span >\\D</span></td><td>A non digit</td><td>\\D\\D\\D</td><td>ABC</td></tr>\n",
    "\n",
    "<tr ><td><span >\\W</span></td><td>Non-alphanumeric</td><td>\\W\\W\\W\\W\\W</td><td>*-+=)</td></tr>\n",
    "\n",
    "<tr ><td><span >\\S</span></td><td>Non-whitespace</td><td>\\S\\S\\S\\S</td><td>Yoyo</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'r' at the start of the pattern string designates a python \"raw\" string which passes through backslashes without change which is very handy for regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"My telephone number is 408-555-1234\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone = re.search(r'\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d',text)\n",
    "# it is a good habit to keep r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(23, 35), match='408-555-1234'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'408-555-1234'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone.group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table ><tr><th>Character</th><th>Description</th><th>Example Pattern Code</th><th >Exammple Match</th></tr>\n",
    "\n",
    "<tr ><td><span >+</span></td><td>Occurs one or more times</td><td>\tVersion \\w-\\w+</td><td>Version A-b1_1</td></tr>\n",
    "\n",
    "<tr ><td><span >{3}</span></td><td>Occurs exactly 3 times</td><td>\\D{3}</td><td>abc</td></tr>\n",
    "\n",
    "\n",
    "\n",
    "<tr ><td><span >{2,4}</span></td><td>Occurs 2 to 4 times</td><td>\\d{2,4}</td><td>123</td></tr>\n",
    "\n",
    "\n",
    "\n",
    "<tr ><td><span >{3,}</span></td><td>Occurs 3 or more</td><td>\\w{3,}</td><td>anycharacters</td></tr>\n",
    "\n",
    "<tr ><td><span >\\*</span></td><td>Occurs zero or more times</td><td>A\\*B\\*C*</td><td>AAACC</td></tr>\n",
    "\n",
    "<tr ><td><span >?</span></td><td>Once or none</td><td>plurals?</td><td>plural</td></tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(23, 35), match='408-555-1234'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'\\d{3}-\\d{3}-\\d{4}',text )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groups\n",
    "--------\n",
    "What if we wanted to do two tasks, find phone numbers, but also be able to quickly extract their area code (the first three digits). We can use groups for any general task that involves grouping together regular expressions (so that we can later break them down).\n",
    "\n",
    "Using the phone number example, we can separate groups of regular expressions using parentheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_pattern = re.compile(r'(\\d{3})-(\\d{3})-(\\d{4})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = re.search(phone_pattern,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(23, 35), match='408-555-1234'>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'408-555-1234'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'408-555-1234'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'408'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'555'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.group(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1234'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.group(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(5, 8), match='man'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r\"man|woman\",\"This man was here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(5, 10), match='woman'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r\"man|woman\",\"This woman was here.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wildcard Character\n",
    "---------------------------------\n",
    "Use a \"wildcard\" as a placement that will match any character placed there. You can use a simple period . for this. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sai', 'sam', 'son', 'sun']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r's.{2}','sai, sam, ram, son, sun, run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r's*i','sai, sam, ram, son, sun, run, s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bat', 'splat']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One or more non-whitespace that ends with 'at'\n",
    "re.findall(r'\\S+at',\"The bat went splat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starts With and Ends With\n",
    "We can use the ^ to signal starts with, and the $ to signal ends with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='45'>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r\"^\\d{2}\",\"45 ppl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(18, 20), match='12'>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r\"\\d{2}$\",\"45 ppl and drinks 12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"45 ppl and drinks 12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = re.findall(r'[\\d]',text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4', '5', '1', '2']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = re.findall(r'[^\\d]',text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'p',\n",
       " 'p',\n",
       " 'l',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'd',\n",
       " ' ',\n",
       " 'd',\n",
       " 'r',\n",
       " 'i',\n",
       " 'n',\n",
       " 'k',\n",
       " 's',\n",
       " ' ']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ppl and drinks ']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'[^\\d]+',text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phrase = 'This is a string! But it has punctuation. How can we remove it?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'string',\n",
       " 'But',\n",
       " 'it',\n",
       " 'has',\n",
       " 'punctuation',\n",
       " 'How',\n",
       " 'can',\n",
       " 'we',\n",
       " 'remove',\n",
       " 'it']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[^!.? ]+',test_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = ' '.join(re.findall('[^!.? ]+',test_phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a string But it has punctuation How can we remove it'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "christmas = \"jingle-bell jingle-bell jingle all the-way\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jingle-bell', 'jingle-bell', 'the-way']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'[\\w]+-[\\w]+',christmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find words that start with cat and end with one of these options: 'fish','nap', or 'claw'\n",
    "text = 'Hello, would you like some catfish?'\n",
    "texttwo = \"Hello, would you like to take a catnap?\"\n",
    "textthree = \"Hello, have you seen this caterpillar?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(27, 34), match='catfish'>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'cat(fish|nap|claw)',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#homework1 - Write a regular expression to extract all mail ids from any text\n",
    "# homework2 - write a regular expression to extract @domain.com and replace it with @datakishiksha.com\n",
    "# homework3 - write a regular expression to remove all punctations from any text\n",
    "# hoework4 - write a regular expression to remove all numebers and dates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contact@tutorialspoint.com', 'feedback@tp.com']\n"
     ]
    }
   ],
   "source": [
    "#homework1 - Write a regular expression to extract all mail ids from any text\n",
    "import re\n",
    "text = \"Please contact us at contact@tutorialspoint.com for further information.\"+\\\n",
    "        \" You can also give feedbacl at feedback@tp.com\"\n",
    "\n",
    "\n",
    "emails = re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", text)\n",
    "print(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@gmail.com\n"
     ]
    }
   ],
   "source": [
    "# homework2 - write a regular expression to extract @domain.com and replace it with @datakishiksha.com\n",
    "import re\n",
    "\n",
    "s = 'My name is Conrad, and blahblah@gmail.com is my email.'\n",
    "\n",
    "domain = re.search(\"@[\\w.]+\", s)\n",
    "\n",
    "\n",
    "print(domain.group())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given String ==>  Sunday, is best : for ! dasy ;\n",
      "After ==> Sunday is best  for  dasy \n"
     ]
    }
   ],
   "source": [
    "# homework3 - write a regular expression to remove all punctations from any text\n",
    "import re\n",
    "\n",
    "test_str = \"Sunday, is best : for ! dasy ;\"\n",
    "\n",
    "print(\"Given String ==>  \" + test_str)\n",
    "\n",
    "res = re.sub(r'[^\\w\\s]', '', test_str)\n",
    "        \n",
    "print(\"After ==> \" + res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample number for the today date are -- and \n"
     ]
    }
   ],
   "source": [
    "# hoework4 - write a regular expression to remove all numebers and dates()\n",
    "import re\n",
    "\n",
    "str = \"Sample number for the today date are 2020-03-20 and 123456\"\n",
    "\n",
    "pattern  = r'[0-9]'\n",
    "\n",
    "mod_string = re.sub(pattern, '', str)\n",
    "\n",
    "\n",
    "print(mod_string)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
